# Guide d‚Äôutilisation complet ‚Äî Kafka Producer / Consumer / Admin + Ingestion MySQL + Pr√©dictions

Ce guide explique **pas √† pas** comment utiliser :
- `src/admin_kafka.py` : **administration des topics Kafka** (create/delete/list)
- `src/kafka_producer.py` : **envoi** des √©v√©nements clean (JSONL) dans Kafka
- `src/kafka_consumer.py` : **consommation**, ingestion en base via `ingest.py` + pr√©dictions via `ingest_predictions.py`
- MySQL via SQLAlchemy ORM : tables `table_customers`, `table_orders`, `table_events`, `table_predictions`

---

## 1) Pr√©-requis

### 1.1 Outils n√©cessaires
- Python (id√©alement 3.10+)
- Kafka (via Docker Compose dans ton projet)
- MySQL (ou MariaDB) accessible via `SQLALCHEMY_DATABASE_URL`
- Les d√©pendances Python install√©es (kafka-python, sqlalchemy, pandas, joblib, etc.)

### 1.2 Fichiers attendus dans le projet
- `src/config.py` doit contenir au minimum :
  - `KAFKA_BOOTSTRAP_SERVERS`
  - `KAFKA_TOPIC`
  - `CLEAN_JSONL_PATH`
  - `SQLALCHEMY_DATABASE_URL`
  - `CLASSIFICATION_MODEL_PATH`
- Le fichier `CLEAN_JSONL_PATH` doit exister : **1 JSON par ligne**
- Le mod√®le ML (pipeline) doit exister : `CLASSIFICATION_MODEL_PATH`

### 1.3 Rappel sur les scripts d‚Äôingestion
- `src/ingest.py` : ing√®re (UPSERT) `Customer`, `Order`, `Event`
- `src/ingest_predictions.py` : ing√®re (UPSERT) `Prediction`

‚úÖ Recommandation : `ingest.py` et `ingest_predictions.py` doivent exposer des fonctions r√©utilisables :
- `ingest_clean_dataframe(df, engine=...)`
- `ingest_predictions_dataframe(df, engine=..., validate_fk=True)`

---

## 2) Comprendre le flux de bout en bout

### 2.1 Le Producer
`kafka_producer.py` :
- lit un fichier **JSON Lines**
- envoie chaque ligne JSON dans Kafka (topic `KAFKA_TOPIC`)
- optionnel : met `event_id` comme **key Kafka** (meilleure coh√©rence par partition)

### 2.2 Le Consumer
`kafka_consumer.py` :
- lit chaque message du topic Kafka
- parse JSON + v√©rifie cl√©s obligatoires
- d√©duplique (par `event_id`)
- construit une ligne ‚Äúclean‚Äù
- appelle :
  - `ingest_clean_dataframe()` ‚Üí Customer/Order/Event
  - mod√®le ML ‚Üí `return_proba`
  - `ingest_predictions_dataframe()` ‚Üí Prediction

### 2.3 Admin
`admin.py` :
- cr√©e, supprime ou liste les topics Kafka
- utile pour ‚Äúreset‚Äù le flux (repartir de z√©ro)

---

## 3) Pr√©parer l‚Äôenvironnement

### 3.1 Lancer Kafka (Docker)
Depuis la racine du projet (selon ton docker-compose) :

```bash
docker compose up -d
```

Puis v√©rifier que Kafka tourne :

```bash
docker ps
```

> Tu dois voir un container Kafka (et souvent Zookeeper / Kraft selon la stack).

### 3.2 V√©rifier la base MySQL
Ton `SQLALCHEMY_DATABASE_URL` doit √™tre correct, ex :
- `mysql+pymysql://user:password@localhost:3306/dbname`
- ou via docker network : `mysql+pymysql://user:password@mysql:3306/dbname`

---

## 4) Administrer le topic Kafka (admin_kafka.py)

### 4.1 Lister les topics
```bash
python -m src.admin_kafka list
```

### 4.2 Cr√©er le topic
```bash
python -m src.admin_kafka create --name "<ton_topic>" --partitions 1 --replication 1
```

üí° Si tu veux utiliser directement la valeur `KAFKA_TOPIC` du `config.py`, tu peux omettre `--name` si `admin_kafka.py` est cod√© avec `default=KAFKA_TOPIC`.

### 4.3 Supprimer le topic (reset)
```bash
python -m src.admin_kafka delete --name "<ton_topic>"
```

Ensuite tu peux recr√©er :
```bash
python -m src.admin_kafka create --name "<ton_topic>" --partitions 1 --replication 1
```

---

## 5) Lancer le Consumer

Le consumer doit √™tre lanc√© **avant** le producer pour voir le flux en direct :

```bash
python -m src.kafka_consumer
```

Tu dois voir un log du type :
- `[OK] Consumer d√©marr√©. En attente de messages...`

---

## 6) Lancer le Producer

Dans un second terminal :

```bash
python -m src.kafka_producer
```

R√©sultat attendu :
- logs indiquant le nombre de messages envoy√©s
- c√¥t√© consumer : logs `[OK] event_id=... ing√©r√© | proba_return=...`

---

## 7) Format attendu des √©v√©nements (JSONL)

Chaque ligne de `CLEAN_JSONL_PATH` doit √™tre un JSON **avec au minimum** :

```json
{
  "event_id": "uuid",
  "order_id": "uuid",
  "customer_customer_id": "uuid"
}
```

Recommand√© (car utile DB + mod√®le ML) :

```json
{
  "event_id": "uuid",
  "event_time": "2026-02-02T10:00:00Z",
  "order_id": "uuid",
  "customer_customer_id": "uuid",
  "customer_country": "FR",
  "order_device": "mobile",
  "order_channel": "web",
  "order_main_category": "fashion",
  "order_n_items": 3,
  "order_basket_value": 120.0,
  "order_shipping_fee": 5.0,
  "order_discount": 10.0,
  "order_order_total": 115.0,
  "order_is_returned": false
}
```

üí° Si certains champs sont absents, le consumer peut appliquer des **valeurs par d√©faut** (ex: `"unknown"`, `0.0`, `False`) pour respecter `nullable=False`.

---

## 8) V√©rifier les donn√©es en base MySQL

### 8.1 Tables attendues
- `table_customers`
- `table_orders`
- `table_events`
- `table_predictions`

### 8.2 V√©rifications SQL rapides
Exemples :

```sql
SELECT COUNT(*) FROM table_customers;
SELECT COUNT(*) FROM table_orders;
SELECT COUNT(*) FROM table_events;
SELECT COUNT(*) FROM table_predictions;
```

Pour contr√¥ler les derni√®res pr√©dictions :

```sql
SELECT event_id, order_id, customer_customer_id, return_proba
FROM table_predictions
ORDER BY event_id DESC
LIMIT 10;
```

---

## 9) D√©pannage (probl√®mes fr√©quents)

### 9.1 Le consumer ignore tous les messages (doublons)
- Si tu relances le producer avec le m√™me JSONL, `event_id` √©tant PK dans `table_events`, le consumer peut d√©tecter que √ßa existe d√©j√† et ‚Äúskip‚Äù.
‚úÖ Solution : reset topic + vider la base ou utiliser de nouveaux event_id.
commande avec admin_kafka.py pour reset le topic.
```bash
python -m src.admin_kafka delete --name "$KAFKA_TOPIC"
python -m src.admin_kafka create --name "$KAFKA_TOPIC" --partitions 1 --replication 1
```

### 9.2 Erreur FK lors de l‚Äôingestion Prediction
Cause : `Prediction.order_id` et `Prediction.customer_customer_id` r√©f√©rencent des tables.
Si l‚Äôorder/customer/event n‚Äôa pas √©t√© ing√©r√© avant la pr√©diction ‚Üí insertion √©choue.

‚úÖ Solution :
- garder l‚Äôordre : ingest (Customer/Order/Event) **avant** Prediction (c‚Äôest ce que fait le consumer)
- activer `validate_fk=True` dans `ingest_predictions_dataframe`

### 9.3 Le producer tourne mais rien n‚Äôarrive c√¥t√© consumer
- mauvais `KAFKA_BOOTSTRAP_SERVERS`
- mauvais topic
- Kafka non accessible depuis ton environnement (docker network)

‚úÖ V√©rifie :
- `python -m src.admin_kafka list`
- logs docker (`docker logs <container_kafka>`)

### 9.4 Le mod√®le ML plante (features manquantes)
Ton pipeline ML peut attendre des colonnes sp√©cifiques.
‚úÖ Solution :
- aligner `build_model_features()` du consumer avec les features utilis√©es √† l‚Äôentra√Ænement

---

## 10) Commandes ‚Äúrecette‚Äù (copier-coller)

### 10.1 Reset complet du topic
```bash
python -m src.admin_kafka delete --name "$KAFKA_TOPIC"
python -m src.admin_kafka create --name "$KAFKA_TOPIC" --partitions 1 --replication 1
```

### 10.2 Lancer consumer
```bash
python -m src.kafka_consumer
```

### 10.3 Lancer producer (autre terminal)
```bash
python -m src.kafka_producer
```

---

## 11) Bonnes pratiques (examen)
- Lancer le consumer **avant** le producer
- Conserver les logs (preuve de fonctionnement)
- D√©montrer :
  - ingestion tables
  - d√©duplication event_id
  - pr√©dictions ins√©r√©es en DB

---

## 12) R√©sum√© rapide
- `admin.py` : cr√©er/supprimer/lister les topics Kafka
- `kafka_producer.py` : envoie les events clean JSONL dans Kafka
- `kafka_consumer.py` : consomme, ing√®re en base via `ingest.py`, calcule proba via mod√®le, ing√®re via `ingest_predictions.py`
- Les tables MySQL se remplissent automatiquement via SQLAlchemy ORM
